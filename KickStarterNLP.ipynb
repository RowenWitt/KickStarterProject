{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c67ae0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle, json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing, opening pickled array, size brought down from 20.4 GBs to .5 GBs pre-pickling\n",
    "with open('PreProcessing/kickXY.pkl', 'rb') as fp:\n",
    "\tbase = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7df3bca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowenwitt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Key for array column names\n",
    "# key = [0'unid', 1'timeline', 2'goalUSD', 3'category', 4'text', 5'state']\n",
    "\n",
    "# Turning array input from scraped data into dataframe\n",
    "bigDF = pd.DataFrame(data=base)\n",
    "\n",
    "# Splitting category 'slug' into category & subcategory\n",
    "bigDF[3] = bigDF[3].str.split('/', n=1)\n",
    "bigDF[6] = [bigDF[3][i][-1] for i in range(len(bigDF))]\n",
    "bigDF[3] = [bigDF[3][i][0] for i in range(len(bigDF))]\n",
    "\n",
    "# Dropping 'live', 'canceled', 'suspended' records and removing dupes (3.6M->221K)\n",
    "states = ['live', 'canceled', 'suspended']\n",
    "for i in range(len(states)):\n",
    "    bigDF.drop(bigDF[bigDF[5] == states[i]].index, inplace=True)\n",
    "\n",
    "bigDFunique = bigDF.drop_duplicates(keep='last')\n",
    "\n",
    "# Dropping unique_id column post dupe-removal\n",
    "bigDFunique.drop(columns=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2f9e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned, de-duped, state-simplified csv (3.6M -> 221K)\n",
    "bigDFunique.to_csv('cleanKick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ade42c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21294</th>\n",
       "      <td>30</td>\n",
       "      <td>7895.704536</td>\n",
       "      <td>publishing</td>\n",
       "      <td>粵語文學期刊\\nCantonese Literature Periodical</td>\n",
       "      <td>successful</td>\n",
       "      <td>literary journals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>24</td>\n",
       "      <td>4008.273100</td>\n",
       "      <td>comics</td>\n",
       "      <td>A survival horror about love, acceptance, deat...</td>\n",
       "      <td>successful</td>\n",
       "      <td>graphic novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29021</th>\n",
       "      <td>60</td>\n",
       "      <td>11000.000000</td>\n",
       "      <td>comics</td>\n",
       "      <td>A graphic novel and unique take on Anthology H...</td>\n",
       "      <td>failed</td>\n",
       "      <td>graphic novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44904</th>\n",
       "      <td>31</td>\n",
       "      <td>1558.756540</td>\n",
       "      <td>comics</td>\n",
       "      <td>An anthology with 3 self-contained shorts of T...</td>\n",
       "      <td>successful</td>\n",
       "      <td>anthologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45339</th>\n",
       "      <td>31</td>\n",
       "      <td>5623.373450</td>\n",
       "      <td>games</td>\n",
       "      <td>The world's first truly travel optimized playi...</td>\n",
       "      <td>successful</td>\n",
       "      <td>playing cards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658378</th>\n",
       "      <td>17</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>art</td>\n",
       "      <td>Back this project!  Or don't. It doesn't matte...</td>\n",
       "      <td>failed</td>\n",
       "      <td>conceptual art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658379</th>\n",
       "      <td>30</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>publishing</td>\n",
       "      <td>How do you make friends as an adult? I created...</td>\n",
       "      <td>failed</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658380</th>\n",
       "      <td>20</td>\n",
       "      <td>3100.000000</td>\n",
       "      <td>art</td>\n",
       "      <td>A 32 card oracle deck filled with ghosts &amp; spi...</td>\n",
       "      <td>successful</td>\n",
       "      <td>illustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658381</th>\n",
       "      <td>30</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>publishing</td>\n",
       "      <td>Mars Awaits! Each piece of artwork shows one s...</td>\n",
       "      <td>failed</td>\n",
       "      <td>calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658382</th>\n",
       "      <td>30</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>photography</td>\n",
       "      <td>My house was burglarized and my laptop and bel...</td>\n",
       "      <td>failed</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211038 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1             2            3  \\\n",
       "21294    30   7895.704536   publishing   \n",
       "21976    24   4008.273100       comics   \n",
       "29021    60  11000.000000       comics   \n",
       "44904    31   1558.756540       comics   \n",
       "45339    31   5623.373450        games   \n",
       "...      ..           ...          ...   \n",
       "3658378  17    600.000000          art   \n",
       "3658379  30  10000.000000   publishing   \n",
       "3658380  20   3100.000000          art   \n",
       "3658381  30   5000.000000   publishing   \n",
       "3658382  30   1970.000000  photography   \n",
       "\n",
       "                                                         4           5  \\\n",
       "21294              粵語文學期刊\\nCantonese Literature Periodical  successful   \n",
       "21976    A survival horror about love, acceptance, deat...  successful   \n",
       "29021    A graphic novel and unique take on Anthology H...      failed   \n",
       "44904    An anthology with 3 self-contained shorts of T...  successful   \n",
       "45339    The world's first truly travel optimized playi...  successful   \n",
       "...                                                    ...         ...   \n",
       "3658378  Back this project!  Or don't. It doesn't matte...      failed   \n",
       "3658379  How do you make friends as an adult? I created...      failed   \n",
       "3658380  A 32 card oracle deck filled with ghosts & spi...  successful   \n",
       "3658381  Mars Awaits! Each piece of artwork shows one s...      failed   \n",
       "3658382  My house was burglarized and my laptop and bel...      failed   \n",
       "\n",
       "                         6  \n",
       "21294    literary journals  \n",
       "21976       graphic novels  \n",
       "29021       graphic novels  \n",
       "44904          anthologies  \n",
       "45339        playing cards  \n",
       "...                    ...  \n",
       "3658378     conceptual art  \n",
       "3658379        young adult  \n",
       "3658380       illustration  \n",
       "3658381          calendars  \n",
       "3658382            animals  \n",
       "\n",
       "[211038 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigDFunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88582350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-e6fc0a584793>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigDFunique[3] = LeCat.transform(bigDFunique[3])\n",
      "<ipython-input-44-e6fc0a584793>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigDFunique[6] = LeSubCat.transform(bigDFunique[6])\n",
      "<ipython-input-44-e6fc0a584793>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigDFunique[5] = LeState.transform(bigDFunique[5])\n"
     ]
    }
   ],
   "source": [
    "# Label encode category, state, subCategory\n",
    "LeCat = LabelEncoder()\n",
    "LeSubCat = LabelEncoder()\n",
    "LeState = LabelEncoder()\n",
    "\n",
    "# Fit encoders on respective columns\n",
    "LeCat.fit(bigDFunique[3])\n",
    "LeSubCat.fit(bigDFunique[6])\n",
    "LeState.fit(bigDFunique[5])\n",
    "\n",
    "# Apply encodings to columns\n",
    "bigDFunique[3] = LeCat.transform(bigDFunique[3])\n",
    "bigDFunique[6] = LeSubCat.transform(bigDFunique[6])\n",
    "bigDFunique[5] = LeState.transform(bigDFunique[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d380ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict maps\n",
    "def ones(imp):\n",
    "    out = []\n",
    "    for i in range(len(imp)):\n",
    "        out.append(i)\n",
    "    \n",
    "    dmap = dict(zip(out, imp))\n",
    "    return dmap\n",
    "\n",
    "StateMap = ones(LeState.classes_)\n",
    "CatMap = ones(LeCat.classes_)\n",
    "SubCatMap = ones(LeSubCat.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b010dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '3d printing',\n",
       " 1: 'academic',\n",
       " 2: 'accessories',\n",
       " 3: 'action',\n",
       " 4: 'animals',\n",
       " 5: 'animation',\n",
       " 6: 'anthologies',\n",
       " 7: 'apparel',\n",
       " 8: 'apps',\n",
       " 9: 'architecture',\n",
       " 10: 'art',\n",
       " 11: 'art books',\n",
       " 12: 'audio',\n",
       " 13: 'bacon',\n",
       " 14: 'blues',\n",
       " 15: 'calendars',\n",
       " 16: 'camera equipment',\n",
       " 17: 'candles',\n",
       " 18: 'ceramics',\n",
       " 19: \"children's books\",\n",
       " 20: 'childrenswear',\n",
       " 21: 'chiptune',\n",
       " 22: 'civic design',\n",
       " 23: 'classical music',\n",
       " 24: 'comedy',\n",
       " 25: 'comic books',\n",
       " 26: 'comics',\n",
       " 27: 'community gardens',\n",
       " 28: 'conceptual art',\n",
       " 29: 'cookbooks',\n",
       " 30: 'country & folk',\n",
       " 31: 'couture',\n",
       " 32: 'crafts',\n",
       " 33: 'crochet',\n",
       " 34: 'dance',\n",
       " 35: 'design',\n",
       " 36: 'digital art',\n",
       " 37: 'diy',\n",
       " 38: 'diy electronics',\n",
       " 39: 'documentary',\n",
       " 40: 'drama',\n",
       " 41: 'drinks',\n",
       " 42: 'electronic music',\n",
       " 43: 'embroidery',\n",
       " 44: 'events',\n",
       " 45: 'experimental',\n",
       " 46: 'fabrication tools',\n",
       " 47: 'faith',\n",
       " 48: 'family',\n",
       " 49: 'fantasy',\n",
       " 50: \"farmer's markets\",\n",
       " 51: 'farms',\n",
       " 52: 'fashion',\n",
       " 53: 'festivals',\n",
       " 54: 'fiction',\n",
       " 55: 'film & video',\n",
       " 56: 'fine art',\n",
       " 57: 'flight',\n",
       " 58: 'food',\n",
       " 59: 'food trucks',\n",
       " 60: 'footwear',\n",
       " 61: 'gadgets',\n",
       " 62: 'games',\n",
       " 63: 'gaming hardware',\n",
       " 64: 'glass',\n",
       " 65: 'graphic design',\n",
       " 66: 'graphic novels',\n",
       " 67: 'hardware',\n",
       " 68: 'hip-hop',\n",
       " 69: 'horror',\n",
       " 70: 'illustration',\n",
       " 71: 'immersive',\n",
       " 72: 'indie rock',\n",
       " 73: 'installations',\n",
       " 74: 'interactive design',\n",
       " 75: 'jazz',\n",
       " 76: 'jewelry',\n",
       " 77: 'journalism',\n",
       " 78: 'kids',\n",
       " 79: 'knitting',\n",
       " 80: 'latin',\n",
       " 81: 'letterpress',\n",
       " 82: 'literary journals',\n",
       " 83: 'literary spaces',\n",
       " 84: 'live games',\n",
       " 85: 'makerspaces',\n",
       " 86: 'metal',\n",
       " 87: 'mixed media',\n",
       " 88: 'mobile games',\n",
       " 89: 'movie theaters',\n",
       " 90: 'music',\n",
       " 91: 'music videos',\n",
       " 92: 'musical',\n",
       " 93: 'narrative film',\n",
       " 94: 'nature',\n",
       " 95: 'nonfiction',\n",
       " 96: 'painting',\n",
       " 97: 'people',\n",
       " 98: 'performance art',\n",
       " 99: 'performances',\n",
       " 100: 'periodicals',\n",
       " 101: 'pet fashion',\n",
       " 102: 'photo',\n",
       " 103: 'photobooks',\n",
       " 104: 'photography',\n",
       " 105: 'places',\n",
       " 106: 'playing cards',\n",
       " 107: 'plays',\n",
       " 108: 'poetry',\n",
       " 109: 'pop',\n",
       " 110: 'pottery',\n",
       " 111: 'print',\n",
       " 112: 'printing',\n",
       " 113: 'product design',\n",
       " 114: 'public art',\n",
       " 115: 'publishing',\n",
       " 116: 'punk',\n",
       " 117: 'puzzles',\n",
       " 118: 'quilts',\n",
       " 119: 'r&b',\n",
       " 120: 'radio & podcasts',\n",
       " 121: 'ready-to-wear',\n",
       " 122: 'residencies',\n",
       " 123: 'restaurants',\n",
       " 124: 'robots',\n",
       " 125: 'rock',\n",
       " 126: 'romance',\n",
       " 127: 'science fiction',\n",
       " 128: 'sculpture',\n",
       " 129: 'shorts',\n",
       " 130: 'small batch',\n",
       " 131: 'social practice',\n",
       " 132: 'software',\n",
       " 133: 'sound',\n",
       " 134: 'space exploration',\n",
       " 135: 'spaces',\n",
       " 136: 'stationery',\n",
       " 137: 'tabletop games',\n",
       " 138: 'taxidermy',\n",
       " 139: 'technology',\n",
       " 140: 'television',\n",
       " 141: 'textiles',\n",
       " 142: 'theater',\n",
       " 143: 'thrillers',\n",
       " 144: 'toys',\n",
       " 145: 'translations',\n",
       " 146: 'typography',\n",
       " 147: 'vegan',\n",
       " 148: 'video',\n",
       " 149: 'video art',\n",
       " 150: 'video games',\n",
       " 151: 'wearables',\n",
       " 152: 'weaving',\n",
       " 153: 'web',\n",
       " 154: 'webcomics',\n",
       " 155: 'webseries',\n",
       " 156: 'woodworking',\n",
       " 157: 'workshops',\n",
       " 158: 'world music',\n",
       " 159: 'young adult',\n",
       " 160: 'zines'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubCatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65bccbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Tokenizer\n",
    "tok = Tokenizer()\n",
    "\n",
    "# Fitting tokenizer on df text array\n",
    "tok.fit_on_texts(texts=bigDFunique[4])\n",
    "\n",
    "# Getting word index from tokenizer\n",
    "dex = tok.word_index\n",
    "\n",
    "# Keep work index to 1000 words\n",
    "tok.num_words = 1000\n",
    "\n",
    "# Creating sequences from df text array\n",
    "seqs = tok.texts_to_sequences(bigDFunique[4])\n",
    "max_features = len(dex.values()) + 1\n",
    "maxlen = 43\n",
    "\n",
    "# Saving tokenizer to pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Padding sequence length\n",
    "bigX = sequence.pad_sequences(seqs, maxlen)\n",
    "\n",
    "# Creating list of reconstructed text\n",
    "recons = tok.sequences_to_texts(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89e0acff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1f749c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211038, 6)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigDFunique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2be7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-b0ee71e790d5>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigDFunique[j] = [bigX[i][f] for i in range(len(bigDFunique))]\n"
     ]
    }
   ],
   "source": [
    "# Giving text encodings (bigX) own column 7-49\n",
    "for j in range(7, (50)):\n",
    "    f = j-7\n",
    "    bigDFunique[j] = [bigX[i][f] for i in range(len(bigDFunique))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b071d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping out column 4 (unencoded text), re-naming columns, renaming df to df\n",
    "df = bigDFunique.drop(columns=[4])\n",
    "\n",
    "newColumns = [i for i in range(48)]\n",
    "df.columns = newColumns\n",
    "\n",
    "# Splitting into X and Y (4 = success/failure)\n",
    "target = 3\n",
    "X = df.drop(columns=target)\n",
    "Y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d1a0230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((147726, 47), (147726,)), ((63312, 47), (63312,)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating X,Y Train and X,Y Validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "((X_train.shape, y_train.shape),(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c763f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train to numpy array\n",
    "arr = X_train.to_numpy()\n",
    "# Reshape numpy array into 3D tensor\n",
    "tarr = arr.reshape(147726, 47, 1)\n",
    "# Convert ints to floats\n",
    "X_train = np.asarray(tarr).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "\n",
    "# Convert X_test to numpy array\n",
    "arr = X_test.to_numpy()\n",
    "# Reshape numpy array into 3D tensor\n",
    "varr = arr.reshape(63312, 47, 1)\n",
    "# Convert ints to floats\n",
    "X_test = np.asarray(varr).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b91a7c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((147726, 47, 1), (147726,)), ((63312, 47, 1), (63312,)))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4e0d573d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131099"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a1b1e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.8000000e+01],\n",
       "        [1.5000000e+03],\n",
       "        [1.0000000e+01],\n",
       "        ...,\n",
       "        [1.0000000e+01],\n",
       "        [4.4300000e+02],\n",
       "        [3.2000000e+01]],\n",
       "\n",
       "       [[4.8000000e+01],\n",
       "        [3.3951188e+04],\n",
       "        [8.0000000e+00],\n",
       "        ...,\n",
       "        [3.2000000e+02],\n",
       "        [3.8000000e+01],\n",
       "        [6.1000000e+01]],\n",
       "\n",
       "       [[3.0000000e+01],\n",
       "        [1.0000000e+03],\n",
       "        [7.0000000e+00],\n",
       "        ...,\n",
       "        [7.6400000e+02],\n",
       "        [1.0300000e+02],\n",
       "        [3.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[4.0000000e+01],\n",
       "        [1.5000000e+04],\n",
       "        [7.0000000e+00],\n",
       "        ...,\n",
       "        [8.5400000e+02],\n",
       "        [2.2000000e+01],\n",
       "        [6.8100000e+02]],\n",
       "\n",
       "       [[4.4000000e+01],\n",
       "        [5.0000000e+03],\n",
       "        [4.0000000e+00],\n",
       "        ...,\n",
       "        [3.0000000e+00],\n",
       "        [5.0000000e+00],\n",
       "        [8.9800000e+02]],\n",
       "\n",
       "       [[3.0000000e+01],\n",
       "        [2.3473076e+03],\n",
       "        [2.0000000e+00],\n",
       "        ...,\n",
       "        [3.8000000e+01],\n",
       "        [2.0000000e+00],\n",
       "        [8.7000000e+01]]], dtype=float32)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "795f12fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "print('building model')\n",
    "maxlen = 43\n",
    "dim = len(dex)\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\" Changes made\n",
    "- Try dropping out unid > Holy shit it broke the model, was it keying on unid?\n",
    "    Nope, I'm an idiot and it frameshifted one to left (durrrr)\n",
    "- Added second LSTM layer, has not really impacted accuracy\n",
    "- Added dense layer\n",
    "\"\"\"\n",
    "\n",
    "# Try embeddings (will it work if only 43/49 columns are encoded words?)\n",
    "\n",
    "# Try normalizing inputs (just words? everything individually?)\n",
    "\n",
    "# Try playing with layers (Add'l dense layer?  Add'l LSTM layer?)\n",
    "\n",
    "# model.add(Embedding(input_dim=15000,\n",
    "# \t\t\t\t\tembeddings_initializer=\"uniform\", output_dim=256))\n",
    "\n",
    "model.add(LSTM(64, input_shape=(47, 1), dropout=0.1, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(64, input_shape=(47, 1), return_sequences=True))\n",
    "\n",
    "model.add(LSTM(64, input_shape=(47, 1), return_sequences=False))\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "52b701db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "\n",
    "# # with a Sequential model\n",
    "# get_3rd_layer_output = K.function([model.layers[3].input],\n",
    "#                                   [model.layers[0].output])\n",
    "# layer_output = get_3rd_layer_output([0])[0]\n",
    "\n",
    "# print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "404ca407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "Epoch 1/100\n",
      "1155/1155 [==============================] - 102s 85ms/step - loss: 0.6591 - accuracy: 0.6138 - val_loss: 0.6495 - val_accuracy: 0.6288\n",
      "Epoch 2/100\n",
      "1155/1155 [==============================] - 97s 84ms/step - loss: 0.6439 - accuracy: 0.6291 - val_loss: 0.6347 - val_accuracy: 0.6412\n",
      "Epoch 3/100\n",
      "1155/1155 [==============================] - 102s 88ms/step - loss: 0.6343 - accuracy: 0.6368 - val_loss: 0.6213 - val_accuracy: 0.6503\n",
      "Epoch 4/100\n",
      "1155/1155 [==============================] - 113s 98ms/step - loss: 0.6170 - accuracy: 0.6510 - val_loss: 0.6401 - val_accuracy: 0.6522\n",
      "Epoch 5/100\n",
      "1155/1155 [==============================] - 111s 96ms/step - loss: 0.6004 - accuracy: 0.6667 - val_loss: 0.6585 - val_accuracy: 0.6594\n",
      "Epoch 6/100\n",
      " 777/1155 [===================>..........] - ETA: 34s - loss: 0.5937 - accuracy: 0.6718"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-c3719f363547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(tarr, y_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "history = model.fit(tarr, y_train,\n",
    "\t\t\t\t   validation_data=(varr, y_test),\n",
    "\t\t\t\t   batch_size=128,\n",
    "\t\t\t\t   epochs=100,\n",
    "\t\t\t\t   verbose=1,\n",
    "                   workers=10)\n",
    "\n",
    "print(history)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bf68a68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn, lstm_cell_16_layer_call_and_return_conditional_losses, lstm_cell_15_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save first model (get a point on the board!)\n",
    "model.save('models/model2')\n",
    "print('saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "05e0828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Convert model to tflite model (it can't be that simple)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('models')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Resave model\n",
    "with open('model1.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take forever to run, accuracy is so-so for 3-category class\n",
    "# Options are\n",
    "#    1. Sample data set, 3,600,000 is too many (weak)\n",
    "#    2. Set up EC2 instance, pay like 10 dollars/hour (cool, difficult)\n",
    "#    3. Increase batch-size to the 10k range (dumb?)\n",
    "#    4. Decrease validation size to 1/5\n",
    "#                Test set size = 1,830,912 ... 1/5 = 366,182.4\n",
    "# Blurb dataset is 150Char, Descrip dataset is 3,500Char\n",
    "# Will this impact training time?  Maybe not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6aa272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
